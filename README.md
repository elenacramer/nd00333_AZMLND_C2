# Training-Deploying-Consuming ML models using AzureML

In this project we will demonstrate an end-to-end ML example, that is, from model training to model deployment to consuming a model. Although we will train a model, our focus will be on deployment and consumption.  

The data sets is the same as for the previous project, i.e. the data set contains information such as age, marital status, job, education ect. of bank clients and the goal is to predict whether a client will subscribe to a term deposit or not, described by the variable "y". 

![dataset](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/registered_dataset.png)

Two approaches where applied. First one using Automated ML and the second using Python SDK to create a pipeline with AutoML steps. The apporaches have two things in common; both require loading the data set and both use AutoML to train and retrieve the best model. 

## Automated ML 
The required steps for training, deploying and consuming a model using Automated ML:
- **Create a new Automated ML run & experiment** 
	- Configure a new compute cluster
		- Select Virtual Machine, the minimum and maximum number of nodes
	- Select best model for deployment
- **Deploy the best model** 
	- Enable Authentication and use Azure Container Instance (ACI)
	- Enable Application Insights: 
		- Running the script *logs.py* enables to view the logs 
- **Consume deployed model**
	- Using *Swagger* 
		- Download the *swagger.json* file (can be found under the Endpoint section -> deployed model) 
		- Must be downloaded to the same directory as *swagger.sh* and *serve.py*
		-  Run swagger.sh and sesrve.py 
	- Using a *deployed service via an HTTP API* ( that is a URL that is exposed over the network so that interaction with a trained model can happen via HTTP requests)
		-  Interact with the endpoint via running the script *endpoint.py* 
			- To run the script the "scoring uri" and a "key" must be provided (generated by the deployed model)
  
 ### Key Steps 
- Experiment is completed:
![experiment_completed](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/completed_AutomatedML_run.png)

- Models of the Automated ML run:
![models](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/Automated_ML_models.png)


- Best model of the completed Automated ML run:
![best_model](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/AutomatedML_best_model.png)

- Application insights is a useful tool to detect anomalies and visualize performance. Running the script *logs.py* provides informational output produced by the software, i.e. logs. 

Here we can see that the application insights is enabled:
![app_insights](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/AutomatedML_app_insights.png)
and the logs: 
![Logs](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/running_logs_py.png)

- Swagger is a tool that helps build, document, and consume RESTful web services.*swagger.sh* runs the swagger UI container and makes it available on port 9000. After the Swagger UI container is running, we can access the website on http://localhost:9000. Running *serve.py* enables the contents of swagger.json to be consumed locally by Swagger. It will run and serve contents on localhost:8000. Here we can see Swagger running on localhost showing the HTTP API methods: 
![swagger](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/swagger.png)

- We can interact with the endpoint using the script *endpoint.py*. It contains data about two clients we want to make a prediction on. By running the scripts we obtain the desired predictions:
![model_pred](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/model_pred.png)

## Pipeline with AutoML Steps
The required steps using Python SDK and *Pipeline* object: 
- Crreate an experiment and attach an AmlCompute cluster 
- Train
	- Create AutoMLConfig  object
- **Create a Pipeline**  
	- Load data set from datastore
	- Create AutoMLStep
	- Create Pipeline object 
	- Submit experiment 
	- Retrieve best model 
- **Publish** and run from REST endpoint

### Key Steps
- Pipeline is created:
 ![pipeline_created](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/pipeline_created.png) 
 
- Pipeline Endpoint:
![pipeline_endpoint](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/pipeline_endpoint.png)

- Pipeline is published ( has a REST endpoint and a status of ACTIVE):
![piblished_pipeline](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/pipeline_rest_endpoint.png)

- Running steps using the *RunDetails Widget* in the notebbok:
![running_steps_notebook](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/run_details_notebook.png)  
  
- Scheduled / completed run
![scheduled_run](https://github.com/elenacramer/nd00333_AZMLND_C2/blob/master/screenshots/pipeline_sheduled_run.png)


## Screen Recording
[Screencast](https://youtu.be/nkuXK5pUuqY)

